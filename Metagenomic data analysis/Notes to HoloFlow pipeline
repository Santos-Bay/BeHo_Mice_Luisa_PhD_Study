##### Holoflow Pipeline #####
#D. 13/08/2021
#Meeting with Antton and Adam


#Antton's new PhD student, Raphael, will be in charge of the HoloFlow and Computerome.
#When we have Qs and cannot find the answers, we ask Raphael

#Antton will create a Computerome account to me. He asks Thonmas, since he's the admin of new accounts at KU.
#You have to use Computerome to run the HoloFlow

#To Log In to Computerome:
ssh [username]@ssh.computerome.dk

#iTerm version 3.4 can be used instead of Terminal
#Downloaded now
#Use iTerm to run Computerome

#If needing help in regards of Computerome - ask computerome@dtu.dk
#Or this website: Computerome.dk
#To discuss Computerome stuff, go here:
#https://stackoverflow.com/c/computerome/questions

#Go through the tutorial/wiki to Computerome


#Snakemake is currently the place where the HoloFlow is "installed". It's not a python package you can install atm.

#Update Holoflow, when/if not having used it for 2 months or so

######
#Notes to the scripts
######


########## beho_HF_0-Preparation.sh ##########
#Keep your work directory outside of the HoloFlow directory, but you can have the HoloFlow inside of your work directory. This way you know which "version" of Holoflow you've used for this certain analysis.
#Download HoloFlow and create directories
#Don't change the name of the directories/folders written in the .sh script. This is to help Raphael; easier for him to help all of us


########## beho_HF_1-PrepareGenome.sh #########
#Download the ref genomes of interest e.g. Vulpes vulpes and mus musculus
#Antton recommended to also download the human genome to map against it as well (as a control)
#The job script is the actual running of HoloFlow
#Keep the "PRG.txt" in the script - don't change it
#Get a timestamp to add to the output files, so it does not overwrite the old created output "PRG" files


########## beho_HF_2-Preprocessing.sh #########
#Input files are the raw data
#Result/output: Filter, trim etc. and end up having reads of that are host mapped (.bam) e.g.
#line 19: Add only one ind= (Antton had several, but don't do that yourself)
#same goes for line 41: add only one.
#from line 66 you can run up to 10 samples/jobs simultaneously. Not more!! But it is better to run just one job = one sample --> finishes --> delete the input files (R1 and R2) --> run a new job

#softlinks = an alias of a file


########## beho_HF_3-Coassembly.sh #########
#Input files are the filtered, trimmed, QC-checked, dereplicated, NON-host mapped reads
#Result/output: co-assembled MAGs
#Only input one sample at the time - thus, run one at the time
#After running the DASTool manually, remove the modules loaded. If not, you may risk having issues running the HoloFlow


########## beho_HF_4-Dereplication.sh #########
#dereplicated = compressing?
#Input: All samples from the co-assembly step. Thus, wait running this until all samples/invidivuals have been processed
#Result/output: Unique MAGs and a MAG catalogue (one fasta file per MAG)
#Only do the co-assembly, when the MAG catalogue is still in the making
#For BeHo, we will most likely be done with the catalogue after the 2nd and 3rd batch is processed
#The script assess how alike the MAGs are. If very closely related/almost identical, only one MAG is kept (the best)
#Moreover, we assign the taxanomy, create a phylogenetic tree and match the function to a gene


########## beho_HF_5-Abundances.sh ##########
#Result/output: Relative abundances & statistics
#Computational demanding (3 days to run)


########## beho_HF_6-TransferToERDA.sh ##########
#You transfer all the output files to ERDA
#Then, you remove all the files




#### TO DO ####
#Set up Computerome
#Play around in the Computerome
#Run one individual = 11 treatment/samples each
#Adam and Raphael will get an individual each as well
#Find the mice in Pool 2
